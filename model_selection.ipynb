{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is the part of Georgetown University Data Science Project - Team Ship Happen\n",
    "\n",
    "\n",
    "## Purpose of this notebook is Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spaul\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Standard Python libraries\n",
    "import os                                    # For accessing operating system functionalities\n",
    "import json                                  # For encoding and decoding JSON data\n",
    "import pickle                                # For serializing and de-serializing Python objects\n",
    "\n",
    "# Libraries that can be pip installed\n",
    "import requests                              # Simple Python library for HTTP\n",
    "import pandas as pd                          # Library for building dataframes similar to those in R\n",
    "import seaborn as sns                        # Statistical visualization library based on Matplotlib\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV, LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, auc, roc_curve, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bunch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- baltimore_knn.txt\n",
      "- baltimore_randf.txt\n",
      "- bokeh_accident.txt\n",
      "- bokeh_dead.txt\n",
      "- bokeh_injured.txt\n",
      "- bokeh_injury.xlsx\n",
      "- bokeh_region.txt\n",
      "- data\n",
      "- feature_selection.ipynb\n",
      "- incident_knn-classifier.pickle\n",
      "- incident_random-forest-classifier.pickle\n",
      "- incident_visualization_bokeh.ipynb\n",
      "- ingetion_wranling.ipynb\n",
      "- LICENSE\n",
      "- meta_incident.json\n",
      "- model evaluation.txt\n",
      "- model_selection.ipynb\n",
      "- model_selection_categorical.ipynb\n",
      "- mvinjury.txt\n",
      "- mvinjury.zip\n",
      "- mvinjury_data.txt\n",
      "- mvinjury_data_final.txt\n",
      "- predicton comparision.xlsx\n",
      "- README.md\n",
      "- results_user_input_data_random-forest-classifier.txt\n",
      "- result_baltimore_knn.txt\n",
      "- result_baltimore_randf.txt\n",
      "- user_input_data.txt\n",
      "- Vessel_age_dist.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_DIR = os.path.abspath(os.path.join(\".\", \"..\", \"ship-happens\"))\n",
    "\n",
    "# Show the contents of the data directory\n",
    "for name in os.listdir(DATA_DIR):\n",
    "    if name.startswith(\".\"): continue\n",
    "    print(\"- {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260364, 6)\n",
      "(260364,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(root=DATA_DIR):\n",
    "    # Construct the `Bunch` for the Misle incident dataset\n",
    "    filenames     = {\n",
    "        'meta': os.path.join(root, 'meta_incident.json'),\n",
    "        'rdme': os.path.join(root, 'ReadMe.md'),        \n",
    "        'data': os.path.join(root, 'mvinjury_data_final.txt')        \n",
    "    }\n",
    "\n",
    "    # Load the meta data from the meta json\n",
    "    with open(filenames['meta'], 'r') as f:\n",
    "        meta = json.load(f)\n",
    "        target_names  = meta['target_names']\n",
    "        feature_names = meta['feature_names']\n",
    "\n",
    "    # Load the description from the README. \n",
    "    with open(filenames['rdme'], 'r') as f:\n",
    "        DESCR = f.read()\n",
    "\n",
    "    # Load the dataset from the text file.\n",
    "    mydataset = np.loadtxt(filenames['data'])\n",
    "\n",
    "    # Extract the target from the data\n",
    "    data   = mydataset[:, 0:-1]\n",
    "    target = mydataset[:, -1]\n",
    "\n",
    "    # Create the bunch object\n",
    "    return Bunch(\n",
    "        data=data,\n",
    "        target=target,\n",
    "        filenames=filenames,\n",
    "        target_names=target_names,\n",
    "        feature_names=feature_names,\n",
    "        DESCR=DESCR\n",
    "    )\n",
    "\n",
    "# Save the dataset as a variable we can use.\n",
    "mydataset = load_data()\n",
    "\n",
    "print(mydataset.data.shape)\n",
    "print(mydataset.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_evaluate(dataset, model, label, **kwargs):\n",
    "    \n",
    "    start  = time.time() # Start the clock! \n",
    "    scores = {'precision':[], 'recall':[], 'accuracy':[], 'f1':[]}\n",
    "    \n",
    "    for train, test in KFold(mydataset.data.shape[0], n_folds=12, shuffle=True):\n",
    "        X_train, X_test = mydataset.data[train], mydataset.data[test]\n",
    "        y_train, y_test = mydataset.target[train], mydataset.target[test]\n",
    "        \n",
    "        estimator = model(**kwargs)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        expected  = y_test\n",
    "        predicted = estimator.predict(X_test)\n",
    "        \n",
    "        # Append our scores to the tracker\n",
    "        scores['precision'].append(metrics.precision_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['recall'].append(metrics.recall_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['accuracy'].append(metrics.accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(metrics.f1_score(expected, predicted, average=\"weighted\"))\n",
    "\n",
    "    # Report\n",
    "    print(\"Build and Validation of {} took {:0.3f} seconds\".format(label, time.time()-start))\n",
    "    print(\"Validation scores are as follows:\\n\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    \n",
    "    # Write official estimator to disk\n",
    "    estimator = model(**kwargs)\n",
    "    estimator.fit(mydataset.data, mydataset.target)\n",
    "    \n",
    "    outpath = label.lower().replace(\" \", \"-\") + \".pickle\"\n",
    "    with open(outpath, 'wb') as f:\n",
    "        pickle.dump(estimator, f)\n",
    "\n",
    "    print(\"\\nFitted model written to:\\n{}\".format(os.path.abspath(outpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform SVC Classification\n",
    "#fit_and_evaluate(mydataset, SVC, \"Incident_SVM Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Validation of Incident_KNN Classifier took 23.643 seconds\n",
      "Validation scores are as follows:\n",
      "\n",
      "accuracy     0.984913\n",
      "f1           0.977697\n",
      "precision    0.972265\n",
      "recall       0.984913\n",
      "dtype: float64\n",
      "\n",
      "Fitted model written to:\n",
      "C:\\project\\ship-happens\\incident_knn-classifier.pickle\n"
     ]
    }
   ],
   "source": [
    "# Perform kNN Classification\n",
    "fit_and_evaluate(mydataset, KNeighborsClassifier, \"Incident_KNN Classifier\", n_neighbors=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Validation of Incident_Random Forest Classifier took 43.656 seconds\n",
      "Validation scores are as follows:\n",
      "\n",
      "accuracy     0.981315\n",
      "f1           0.976906\n",
      "precision    0.973188\n",
      "recall       0.981315\n",
      "dtype: float64\n",
      "\n",
      "Fitted model written to:\n",
      "C:\\project\\ship-happens\\incident_random-forest-classifier.pickle\n"
     ]
    }
   ],
   "source": [
    "# Perform Random Forest Classification\n",
    "fit_and_evaluate(mydataset, RandomForestClassifier, \"Incident_Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spaul\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\spaul\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Validation of Incident_Logistic Regression took 43.749 seconds\n",
      "Validation scores are as follows:\n",
      "\n",
      "accuracy     0.985098\n",
      "f1           0.977722\n",
      "precision    0.970456\n",
      "recall       0.985098\n",
      "dtype: float64\n",
      "\n",
      "Fitted model written to:\n",
      "C:\\project\\ship-happens\\incident_logistic-regression.pickle\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "# Perform Logistic Regression\n",
    "fit_and_evaluate(mydataset, LogisticRegression, \"Incident_Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Validation of Incident_ExtraTrees Classifier took 25.629 seconds\n",
      "Validation scores are as follows:\n",
      "\n",
      "accuracy     0.979444\n",
      "f1           0.976221\n",
      "precision    0.973346\n",
      "recall       0.979444\n",
      "dtype: float64\n",
      "\n",
      "Fitted model written to:\n",
      "C:\\project\\ship-happens\\incident_extratrees-classifier.pickle\n"
     ]
    }
   ],
   "source": [
    "# Perform ExtraTreesClassifier\n",
    "fit_and_evaluate(mydataset, ExtraTreesClassifier, \"Incident_ExtraTrees Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Validation of Incident_Bagging Classifier took 89.055 seconds\n",
      "Validation scores are as follows:\n",
      "\n",
      "accuracy     0.980358\n",
      "f1           0.976703\n",
      "precision    0.973538\n",
      "recall       0.980358\n",
      "dtype: float64\n",
      "\n",
      "Fitted model written to:\n",
      "C:\\project\\ship-happens\\incident_bagging-classifier.pickle\n"
     ]
    }
   ],
   "source": [
    "# Perform BaggingClassifier\n",
    "fit_and_evaluate(mydataset, BaggingClassifier, \"Incident_Bagging Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Validation of Incident_Gaussian NB took 1.139 seconds\n",
      "Validation scores are as follows:\n",
      "\n",
      "accuracy     0.955608\n",
      "f1           0.963438\n",
      "precision    0.971800\n",
      "recall       0.955608\n",
      "dtype: float64\n",
      "\n",
      "Fitted model written to:\n",
      "C:\\project\\ship-happens\\incident_gaussian-nb.pickle\n"
     ]
    }
   ],
   "source": [
    "# Perform GaussianNB\n",
    "fit_and_evaluate(mydataset, GaussianNB, \"Incident_Gaussian NB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spaul\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\spaul\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Validation of Incident_SGD Classifier took 2.842 seconds\n",
      "Validation scores are as follows:\n",
      "\n",
      "accuracy     0.953722\n",
      "f1           0.958261\n",
      "precision    0.971249\n",
      "recall       0.953722\n",
      "dtype: float64\n",
      "\n",
      "Fitted model written to:\n",
      "C:\\project\\ship-happens\\incident_sgd-classifier.pickle\n"
     ]
    }
   ],
   "source": [
    "# Perform SGDClassifier\n",
    "fit_and_evaluate(mydataset, SGDClassifier, \"Incident_SGD Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Take user data from text file to predict accident (yes/no), using KNN model\n",
    "import csv\n",
    "\n",
    "def load_model(path='incident_knn-classifier.pickle'):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "# Create a reader for the text file and a write to write output \n",
    "with open('baltimore_knn.txt', 'r') as fin:\n",
    "    reader = csv.reader(fin, delimiter='\\t') \n",
    "\n",
    "    # Create writer to write CSV output \n",
    "    with open('result_baltimore_knn.txt', 'w') as fout:\n",
    "        writer = csv.writer(fout) \n",
    "\n",
    "        # Go through all your data and run the predictions, writing to the results\n",
    "        for idx, row in enumerate(reader):\n",
    "            accident = model.predict([row]) \n",
    "            writer.writerow([idx+1,row[0], accident])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take user data from text file to predict accident (yes/no), using random forest classifier model\n",
    "import csv\n",
    "\n",
    "def load_model(path='incident_random-forest-classifier.pickle'):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "# Create a reader for the text file and a write to write output \n",
    "with open('baltimore_randf.txt', 'r') as fin:\n",
    "    reader = csv.reader(fin, delimiter='\\t') \n",
    "\n",
    "    # Create writer to write CSV output \n",
    "    with open('result_baltimore_randf.txt', 'w') as fout:\n",
    "        writer = csv.writer(fout) \n",
    "\n",
    "        # Go through all your data and run the predictions, writing to the results\n",
    "        for idx, row in enumerate(reader):\n",
    "            accident = model.predict([row]) \n",
    "            writer.writerow([idx+1,row[0], accident])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
